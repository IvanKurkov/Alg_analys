{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQCSzN66OtP8"
   },
   "source": [
    "### Практическое задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Обучить любую модель классификации на датасете IRIS до применения PCA и после него. Сравнить качество классификации по отложенной выборке.\n",
    "- Написать свою реализацию метода главных компонент с помощью сингулярного разложения с использованием функции numpy.linalg.svd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_frame = pd.DataFrame(iris.data)\n",
    "iris_frame.columns = iris.feature_names \n",
    "iris_frame['target'] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris_frame[iris.feature_names], iris_frame['target'], test_size=0.3 ,random_state = 43 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns = iris.feature_names )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns = iris.feature_names )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_no_pca = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model_no_pca.fit(X_train_scaled, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pca_predict = lr_model_no_pca.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, no_pca_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Пробуем использовать самописный PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собственные значения в порядке убывания:\n",
      "(304.4930838102175, array([ 0.52465916, -0.25004775,  0.58180886,  0.56895285]))\n",
      "(98.62361482431878, array([-0.36738691, -0.92845041, -0.01663093, -0.0522504 ]))\n",
      "(14.51797334097651, array([-0.73002103,  0.25137012,  0.16660771,  0.61328968]))\n",
      "(2.3653280244874075, array([ 0.23838819, -0.11076558, -0.79590435,  0.54537906]))\n"
     ]
    }
   ],
   "source": [
    " \n",
    "covariance_matrix = X_train_scaled.T.dot(X_train_scaled)\n",
    "\n",
    "eig_values, eig_vectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "# сформируем список кортежей (собственное значение, собственный вектор)\n",
    "eig_pairs = [(np.abs(eig_values[i]), eig_vectors[:, i]) for i in range(len(eig_values))]\n",
    "\n",
    "# и отсортируем список по убыванию собственных значений\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print('Собственные значения в порядке убывания:')\n",
    "for i in eig_pairs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля дисперсии, описываемая каждой из компонент \n",
      "[72.498353288147, 23.481813053409223, 3.45666031928012, 0.5631733391636682]\n",
      "Кумулятивная доля дисперсии по компонентам \n",
      "[ 72.49835329  95.98016634  99.43682666 100.        ]\n"
     ]
    }
   ],
   "source": [
    "eig_sum = sum(eig_values)\n",
    "var_exp = [(i / eig_sum) * 100 for i in sorted(eig_values, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(f'Доля дисперсии, описываемая каждой из компонент \\n{var_exp}')\n",
    "\n",
    "# а теперя оценим кумулятивную (то есть накапливаемую) дисперсию при учитывании каждой из компонент\n",
    "print(f'Кумулятивная доля дисперсии по компонентам \\n{cum_var_exp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица весов W:\n",
      " [[ 0.52465916 -0.36738691]\n",
      " [-0.25004775 -0.92845041]\n",
      " [ 0.58180886 -0.01663093]\n",
      " [ 0.56895285 -0.0522504 ]]\n"
     ]
    }
   ],
   "source": [
    "# Сформируем вектор весов из собственных векторов, соответствующих первым двум главным компонентам\n",
    "W = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))\n",
    "\n",
    "print(f'Матрица весов W:\\n', W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сформируем новую матрицу \"объекты-признаки\"\n",
    "X_train_scaled_pca = X_train_scaled.dot(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled_pca = X_test_scaled.dot(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_with_pca = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model_with_pca.fit(X_train_scaled_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_pca = lr_model_with_pca.predict(X_test_scaled_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9111111111111111"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, predict_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Попробуем сделать с РСА из коробки, вдруг что-то не так сделали."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_my = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_pca2=pca_my.fit_transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled_pca2 = pca_my.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_with_pca2 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model_with_pca2.fit(X_train_scaled_pca2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_pca2 = lr_model_with_pca2.predict(X_test_scaled_pca2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9111111111111111"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, predict_pca2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### По итогу видим, что качество упало. Оно и понятно, модель и без наших манипуляций давала отличнй результат. (если брать дата сет из коробки и сразу подавать в модель, то выходит 0.977 accuracy). Сокращая размерность мы крадем информацию, и в данном конкретном случае, необходимости в сокращении размерности нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Попробуем понять, как это работает..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица Sigma размер:(105, 4) :\n",
      "[[81.06660922  0.          0.          0.        ]\n",
      " [ 0.         14.72690652  0.          0.        ]\n",
      " [ 0.          0.          2.80672273  0.        ]\n",
      " [ 0.          0.          0.          1.64176788]]\n",
      "Матрица V размер:(4, 4) :\n",
      "[[-0.74950495 -0.28906382  0.51705963 -0.29552289]\n",
      " [-0.38145361 -0.54131265 -0.69383574  0.28295887]\n",
      " [-0.51345687  0.70795958 -0.07464005  0.47914938]\n",
      " [-0.17057994  0.34959398 -0.49565126 -0.77654129]]\n"
     ]
    }
   ],
   "source": [
    "A = X_train.iloc[:,:4]\n",
    "#A\n",
    "U, s, W = np.linalg.svd(A)\n",
    "# Транспонируем матрицу W\n",
    "V = W.T\n",
    "\n",
    "# s - список диагональных элементов, его нужно привести к виду диагональной матрицы для наглядности\n",
    "Sigma = np.zeros_like(A, dtype=float)\n",
    "Sigma[np.diag_indices(min(A.shape))] = s\n",
    "print(f'Матрица Sigma размер:{Sigma.shape} :\\n{Sigma[:4]}')\n",
    "print(f'Матрица V размер:{V.shape} :\\n{V}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица Sigma размер:(105, 4) :\n",
      "[[8.10666092e+01 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.47269065e+01 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.35074188e-14 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.76176254e-15]]\n"
     ]
    }
   ],
   "source": [
    "n_elements = 2\n",
    "Sigma = Sigma[:, :n_elements]\n",
    "\n",
    "V = V[:,:n_elements]\n",
    "# reconstruct\n",
    "B = U.dot(Sigma.dot(V.T))\n",
    "X_train_svd = U.dot(Sigma.dot(V.T)) \n",
    "\n",
    "U, s, W = np.linalg.svd(B)\n",
    "# Транспонируем матрицу W\n",
    "V = W.T\n",
    "\n",
    "Sigma = np.zeros_like(A, dtype=float)\n",
    "Sigma[np.diag_indices(min(A.shape))] = s\n",
    "print(f'Матрица Sigma размер:{Sigma.shape} :\\n{Sigma[0:4]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.87255816e+00,  4.76951053e-01,  2.49070696e-16,\n",
       "         2.16369011e-16],\n",
       "       [-7.57796277e+00,  5.37270993e-01,  2.39665042e-16,\n",
       "         1.89107440e-16],\n",
       "       [-7.23163659e+00, -2.27624190e-01,  5.38962172e-16,\n",
       "        -1.29905225e-16]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.dot(B.T).T[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ну по идее тут мы можем просто откинуть за нуленные компоненты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Попробуем собрать все воедино."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_svd(train, test, k): \n",
    "    U, s, W = np.linalg.svd(train)\n",
    "    U2, s2, W2 = np.linalg.svd(test)\n",
    "    \n",
    "    B = np.dot(U[:,:k],np.dot(np.diag(s[:k]),W[:k,:])) \n",
    "    B2 = np.dot(U2[:,:k],np.dot(np.diag(s2[:k]),W2[:k,:])) \n",
    "    \n",
    "    train_svd = W.dot(B.T).T\n",
    "    test_svd = W.dot(B2.T).T\n",
    "    \n",
    "    return train_svd[:, :k], test_svd[:, :k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_svd, X_test_svd = compress_svd(X_train_scaled, X_test_scaled, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_with_svd = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model_with_svd.fit(X_train_svd, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Качество на трейне"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_svd = lr_model_with_svd.predict(X_train_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9238095238095239"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, predict_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Качество на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_svd = lr_model_with_svd.predict(X_test_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9111111111111111"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, predict_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Используем метод из коробки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_transform = TruncatedSVD(n_components=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_svd2 = svd_transform.fit_transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_svd2 = svd_transform.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_with_svd2 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model_with_svd2.fit(X_train_svd2, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Качество на трейне"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_svd2 = lr_model_with_svd2.predict(X_train_svd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9238095238095239"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, predict_svd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Качество на трейне"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_svd2 = lr_model_with_svd2.predict(X_test_svd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9111111111111111"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, predict_svd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Сложная тема, в виду необходимости хорошего представления линейного пространства и манипуляций с ним. На интуитивном уровне, понятно, что мы сокращая например из 4 признаков в 3, пытаемся информацию о дисперсии 4 признаков \"размазать\" на 3 признака, при этом часть информации всетаки теряется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "cCkVGRl6bIYc",
    "lRXJHtZRbIYe"
   ],
   "name": "Lesson_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
